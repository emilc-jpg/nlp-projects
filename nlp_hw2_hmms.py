# -*- coding: utf-8 -*-
"""NLP HW2 - HMMs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jZLRXqjpg_waJyb34cXlAb0baxCoX-pW
"""

#===============================================================================
# Emily Chen (exc160630)
# CS 6320 Natural Language Processing
# Homework 2: Hidden Markov Models (HMMs)
# Goal: Design and implement a part-of-speech tagger (POST) using
#   a hidden markov model. 
#===============================================================================
# pip install pandas
# pip install numpy
import pandas as pd
import numpy as np
import os
import io
import sys

class HMMTagger():

  def __init__(self):
    self.initial_tag_probability = None
    self.transition_probability = None      # A matrix
    self.emission_probability = None        # B matrix

  #-------------------------------------------------------
  #  FUNCTION: load_corpus
  #   This function loads the corpus at the given path 
  #   and returns a POS-tagged sentence as a list of 2-ples.
  #-------------------------------------------------------
  def load_corpus(self, path):
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', 2000)

    num_files = 0
    num_sentences = 0
    total_tuplelist = []
    temp_list = []

    if not os.path.isdir(path):
      sys.exit("Input path is not a directory") 
    for filename in os.listdir(path):
      filename = os.path.join(path, filename)
      try:
        reader = io.open(filename)
        """
        YOUR CODE GOES HERE: Complete the rest of the method so that it outputs a list of lists as described in the question
        """
        # Open the file, read corpus as input
        num_files += 1
        for line in reader:
          if not line.strip():
            continue
          else:
            tuplelist = []      # List of word-key pairings for a particular sentence

            # Convert the sentence into a list separated by whitespace
            linesplit = line.split() 

            # Split & pair into (word, key) tuples
            #print(linesplit)  ##
            for corpus_pair in linesplit:
              temp_list = corpus_pair.split('/')
              temp_list[0] = temp_list[0].lower()
              tuplelist.append(tuple(temp_list))
            #print(tuplelist)  ##

            total_tuplelist.append(tuplelist)
            num_sentences += 1
    
      except IOError:
        sys.exit("Cannot read file")

    #print("There are " + str(num_files) + " files in the corpus.")
    #print("There are " + str(num_sentences) + " sentences in total.")

    #print("total_tuplelist")  ##
    #print(total_tuplelist[0:10])  ##

    return(total_tuplelist)

  #-------------------------------------------------------
  #  FUNCTION: token_update
  #   Updates dictionary and tables for each token
  #-------------------------------------------------------
  def token_update(self, token_pair, prev_token_pair, tagset, initial_tag_probability, transition_probability, emission_probability, start_sentence):
    #print(token_pair)
    #print(initial_tag_probability)

    if start_sentence: 
      # Update initial tag counts
      initial_tag_probability[token_pair[1]] = initial_tag_probability[token_pair[1]] + 1   
    else:                               
      # Update transition count table from tag to tag
      # Previous token tag + current token tag pairing in cell
      transition_probability.at[prev_token_pair[1], token_pair[1]] += 1 

    # Update emission count table for tag given a certain word
    # If the word already exists in the dictionary, update the count
    # Otherwise, create a new key of ones
    if token_pair[0] in emission_probability:  
      for tag in tagset:
        if token_pair[1] == tag:
          emission_probability[token_pair[0]][tag] = emission_probability[token_pair[0]][tag] + 1
    else:
      emission_probability[token_pair[0]] = dict.fromkeys(tagset, 1)

    return False

  #-------------------------------------------------------
  #  FUNCTION: initialize_probabilities
  #   This method takes as input the list of sentences 
  #   generated by load corpus() and initializes all 
  #   variables required by the tagger.
  #-------------------------------------------------------
  def initialize_probabilities(self, sentences):
    # sentences[0][0][1]) # 1st sentence, 1st word-key pairing, key
    start_sentence = True

    # Create a dictionary for initial tagset counts
    tagset = ["NOUN", "PRONOUN", "VERB", "ADJECTIVE", "ADVERB", "CONJUNCTION", "PREPOSITION", "DETERMINER", "NUMBER", "PUNCT", "X"]
    initial_tag_probability = dict.fromkeys(tagset, 1)
    #print(initial_tag_probability) ##

    # Create initial transition table of tags
    transition_probability = pd.DataFrame(1, columns= tagset, index= tagset)
    #print(transition_probability) ##
    
    # Create initital emission table of tags
    # Dictionary of words : [dictionary of tags : count]
    emission_probability = {}
    #print(emission_probability) ##

    if type(sentences) != list:
      sys.exit("Incorrect input to method")
    """
    YOUR CODE GOES HERE: Complete the rest of the method so that it computes the probability matrices as described in the question
    """

    prev_token_pair = sentences[0][0]

    # Process sentences into probability tables
    for sentence in sentences:  # Iterate through each sentence
      start_sentence = True

      for token_pair in sentence: # Iterate through each tuple in a sentence
        start_sentence = self.token_update(token_pair, prev_token_pair, tagset, initial_tag_probability, transition_probability, emission_probability, start_sentence)
        prev_token_pair = token_pair


    #print(initial_tag_probability)
    #print(transition_probability)
    #print(emission_probability["the"])


    # Probability table calculations
    
    # Total tag frequencies
    tags_sum = transition_probability.sum(axis=1)
    tags_sum = tags_sum.to_dict()

    for item in tags_sum:
      tags_sum[item] = tags_sum[item] + initial_tag_probability[item] - 22
    #print(tags_sum)


    # Initial tag probabilities 
    # [Num times tag appears at beginning of sentence] / ([Num of initial tags (total sentences)] + [Num of total tags])

    for key in initial_tag_probability: 
       initial_tag_probability[key] = initial_tag_probability[key] / (len(sentences) + len(tagset))
    #print(initial_tag_probability)  ##


    # Transition probabilities
    # Probability that a certain tag appears after previous tag
    # [Next | Current tag count] / ([Num times tag occurred] + [Num unique tags])

    transition_probability = transition_probability.astype(float)
    for i in transition_probability.index.values:  # Rows [current]
      for j in transition_probability.columns.values:  # Columns [next]
        # [current|next] / [total times current tag occurred]
        # total times current tag occurred = sum of rows in transition_probabilities + initial_tag_count[i]
        transition_probability.at[i,j] = transition_probability.at[i, j] / (float(tags_sum[i] + len(tagset))) 
    #print(transition_probability) ##

        
    # Emission probabilities
    # Probability that a certain token (word) is generated given a tag
    # [Num of times tag is associated with word] / ([Num of times that tag occurred] + [Num of unique tags])

    for word in emission_probability:
      for tag in emission_probability[word]:
        emission_probability[word][tag] = emission_probability[word][tag] / (float((tags_sum["PUNCT"] + len(tagset)))) 
    #print(emission_probability["the"]) ##

    return(initial_tag_probability, transition_probability, emission_probability)

        
  #-------------------------------------------------------
  #  FUNCTION: viberti_decode
  #   This method returns the most likely tag sequence 
  #   for an input sentence using the Viterbi algorithm
  #-------------------------------------------------------
  def viterbi_decode(self, sentence, initial_tag_probability, transition_probability, emission_probability):
    sentence = sentence.lower()

    if type(sentence) != str:
      sys.exit("Incorrect input to method")
    """
    YOUR CODE GOES HERE: Complete the rest of the method so that it computes the most likely sequence of tags as described in the question
    """
    sentence = sentence.split()
    #print(sentence)

    taggers = []
    tagset = ["NOUN", "PRONOUN", "VERB", "ADJECTIVE", "ADVERB", "CONJUNCTION", "PREPOSITION", "DETERMINER", "NUMBER", "PUNCT", "X"]

    for x in range(len(sentence)):
      possible_prob_states = []
      if x == 0:  
        # Compute first observation tables
        # P(TAG | INITIAL) * P(WORD | TAG)
        for tag in initial_tag_probability:
          if sentence[x] in emission_probability:     # Check if the word exists in the dictionary
            state_probability = initial_tag_probability[tag] * emission_probability[sentence[x]][tag]
          else:
            state_probability = initial_tag_probability[tag] * 0.0001

          possible_prob_states.append(state_probability)    # Add to possible probability states list    
        
        # Calculate the maximum of the possible states
        prob_max = max(possible_prob_states) 

        
        # Get tagger corresponding with the max probability
        state_max = possible_prob_states.index(prob_max)

        taggers.append(tagset[state_max])   # Set to final tag list
        prev_tag = state_max    # Update previous tag

      if x > 0:
        # P(CURRENT TAG | PREVIOUS TAG) * (WORD | TAG) 
        for tag in initial_tag_probability:
          if sentence[x] in emission_probability:
            state_probability = transition_probability[tag][prev_tag] * emission_probability[sentence[x]][tag]
          else:
            state_probability = transition_probability[tag][prev_tag] *  0.0001 
          
          possible_prob_states.append(state_probability)
        
        # Calculate the maximum of the possible states
        prob_max = max(possible_prob_states)

        # Get tagger corresponding with the max probability
        state_max = possible_prob_states.index(prob_max)
        taggers.append(tagset[state_max])
        prev_tag = state_max

    return taggers

"""
#-------------------------------------------------------
#  FUNCTION: main
#   Train language model using training.txt corpus
#-------------------------------------------------------
def main():
  # Create a class object, and load the corpus
  #!unzip train.zip ###
  path = "/content/modified_brown"
  t1 = HMMTagger()
  sentences = t1.load_corpus(path)
  initial_tag_probability, transition_probability, emission_probability = t1.initialize_probabilities(sentences)

  sentence1 = "the planet jupiter and its moons are in effect a mini solar system ."
  print(sentence1)
  print(t1.viterbi_decode(sentence1, initial_tag_probability, transition_probability, emission_probability))

  print("===================================================")

  sentence2 = "computers process programs accurately ."
  print(sentence2)
  print(t1.viterbi_decode(sentence2, initial_tag_probability, transition_probability, emission_probability))


#-------------------------------------------------------
if __name__ == "__main__":
  main()
"""

