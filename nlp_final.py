# -*- coding: utf-8 -*-
"""NLP-Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16foHudxkUtJFqamVHYlxdku_JFwfY8xI
"""

#===============================================================================
# Emily Chen (exc160630)
# CS 6320 Natural Language Processing
# Final Project: Semantic Relation Classification
# Goal: Design and implement a semantic relation classifier using a 
# Bi-LSTM. Given a sentence and two tagged entities,
# determine the relation label to apply
#===============================================================================
# Install packages and libraries
!pip install keras numpy wget
!pip install scikeras
!pip install np_utils
!pip install pyyaml h5py

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.metrics as metrics
import os
import io
import sys
import re
import random

from tensorflow import keras
from keras.utils.np_utils import to_categorical
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report
from collections import defaultdict
from collections import Counter
from random import seed

# To incorpoate the training data
# Upload training zip file, then run this code to unzip.
!unzip dataset.zip

# Data Preprocessing
# Dataset contains 10 total relations
# Preprocess dataset into 5 relations: 
# Cause-Effect, Component-Whole, Entity-Destination, Entity-Origin, and Other-Relation
# Relations not in first 4 categories will be assigned as Other-Relation
#===============================================================================
# Load and preprocess datam returns list of lists, tokenized sentences
# Exclude numerics, entities <e1>, and comments. 
# Replace entities not in list to "Other"
def load_corpus(filename):
  pd.set_option('display.max_rows', None)
  pd.set_option('display.max_columns', None)
  pd.set_option('display.width', 2000)

  stripped_file = []  # List of lines from the text file. 
  data_sentences = [] # Only "original" sentences
                        # Used as preprocessed "original" data for display later if needed.
  data_list = []  # List of lists, each sentence into a list of words
  keys_list = []  # List of keys for each sentence
  relations_list = ["Cause-Effect(e1,e2)", "Cause-Effect(e2,e1)", 
                    "Component-Whole(e1,e2)", "Component-Whole(e2,e1)", 
                    "Entity-Destination(e1,e2)", "Entity-Destination(e2,e1)",
                    "Entity-Origin(e1,e2)", "Entity-Origin(e2,e1)",
                    "Other-Relation"]


  # Convert text file into list of lines
  reader = io.open(filename)
  for line in reader:
    if not line.strip():
      continue
    else: 
      stripped_file.append(line.strip())  # Concatenate spaces
  #print(stripped_file[0:10])  ## 


  # Preprocess lines into data list and keys lists
  stripped_iter = iter(stripped_file)

  for line in stripped_iter:
    # 1 - Line process data (sentence)
    temp = []
    temp = line.split()[1:]   # Remove numbering in original
    temp[0] = temp[0][1:]  # Remove quotation marks from beginning of first word and end of last word
    temp[len(temp)-1] = temp[len(temp)-1][:-1]
    temp = ' '.join(temp)
    data_sentences.append(temp) # Add to list of sentences (original)

    # Clean the text
    text = temp.lower()

    text = text.replace('<e1>', ' _e11_ ')
    text = text.replace('</e1>', ' _e12_ ')
    text = text.replace('<e2>', ' _e21_ ')
    text = text.replace('</e2>', ' _e22_ ')

    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"that's", "that is ", text)
    text = re.sub(r"there's", "there is ", text)
    text = re.sub(r"it's", "it is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)

    text = text.strip()
    data_list.append(text.split())  # Split the sentence by spaces into a list and add to list of sentences

    # 2 - Line process keys
    temp2 = next(stripped_iter)
    if temp2 not in relations_list:   # Preprocess 10 relations into 5
      temp2 = "Other-Relation"
    keys_list.append(temp2)

    # 3 - Skip comment line
    next(stripped_iter)


  # Test original data format
  #print(data_sentences[0:10])  ##
  #print(len(data_sentences))

  return(data_sentences, data_list, keys_list)

#===============================================================================
# Run function here
# Preprocess training data/keys
path1 = "dataset/dataset/SemEval2010_task8_training/TRAIN_FILE.TXT"

train_sentences, train_data, train_keys = load_corpus(path1)

# Data Statistics and Pre-training Analysis

# Sentences head
print("Original sentence: " + train_sentences[0])
print("Preprocessed sentence: " + str(train_data[0]))
print("Label: " + train_keys[0]+ "\n")

# 8000 total training elements
print("Number of training elements: " + str(len(train_data))) 

# 172167 total tokens
total_tokens = 0
max_len = 0
i = 0
while i < len(train_data):
  total_tokens = total_tokens + len(train_data[i])
  if max_len < len(train_data[i]):
    max_len = len(train_data[i])
  i += 1
print("Total number of tokens: " + str(total_tokens))

# 19195 unique tokens
tokentest = sum(train_data, []) # Merge list of lists into one list to use counter
counter = Counter(tokentest)
print("Total number of unique tokens: " + str(len(counter)))

# 89 max sentence length 
print("Maximum length of sentence: " + str(max_len) + "\n")
#print(max(train_keys))

# Postprocessing data statistics
df = pd.DataFrame()
df["Sentence"] = train_data
df["Labels"] = train_keys
print("Training Dataset Head")
display(df[:5])
print("\n")

# Relation Classes Counts total
print("Relation Classes Frequency Expanded")    
freq_df = pd.DataFrame(df["Labels"].value_counts())
freq_df.columns = ["Count"]
display(freq_df)
print("\n")

# Relation Classes Counts Condensed
relations = ["Cause-Effect", "Component-Whole", "Entity-Destination", "Entity-Origin", "Other-Relation"]
relations_d = {"Relation Classes": relations, 
                "Count": [freq_df.Count["Cause-Effect(e1,e2)"] + freq_df.Count["Cause-Effect(e2,e1)"],
                          freq_df.Count["Component-Whole(e1,e2)"] + freq_df.Count["Component-Whole(e2,e1)"], 
                          freq_df.Count["Entity-Destination(e1,e2)"] + freq_df.Count["Entity-Destination(e2,e1)"], 
                          freq_df.Count["Entity-Origin(e1,e2)"] + freq_df.Count["Entity-Origin(e2,e1)"], 
                          freq_df.Count["Other-Relation"]]}

print("Relation Classes Frequency Summary")
freq_df2 = pd.DataFrame(data=relations_d)
display(freq_df2)
print("\n")

freq_df2.plot(x ="Relation Classes", kind = "bar")
plt.title(label="Training Dataset Relation Class Frequency", fontsize=15)
plt.show()

# Tokenize sentences

def create_dataset(sentences, keys):
  # Define relevant lists
  list_x, list_y = list(), list()
  temp1, temp2 = [], []

  # Create dictionary of unique tags
  unique_tags = {
      "PAD": 0,
      "Cause-Effect(e1,e2)": 1,
      "Cause-Effect(e2,e1)":2,
      "Component-Whole(e1,e2)": 3,
      "Component-Whole(e2,e1)": 4,
      "Entity-Destination(e1,e2)": 5,
      "Entity-Destination(e2,e1)":6,
      "Entity-Origin(e1,e2)": 7,
      "Entity-Origin(e2,e1)": 8,
      "Other-Relation": 9,
      "NA": 10
  }
  id = 11

  # Keep track of unique tokens for sentences
  for sentence in sentences:
    for word in sentence:
      if word in unique_tags:
        continue
      else:
        unique_tags[word] = id
        id += 1

  # Tokenize list of sentences
  for sentence in sentences:
    temp1 = []
    for word in sentence:
      temp1.append(unique_tags[word])
    list_x.append(temp1)


  # Tokenize keys 
  for key in keys:
    temp2 = []
    temp2.append(unique_tags[key])
    list_y.append(temp2)
    
  return np.asarray(list_x), np.asarray(list_y), unique_tags

#===============================================================================
train_x, train_y, unique_tags  = create_dataset(train_data, train_keys)

#print(train_x) # train data now tokenized
#print(train_y) # test data now tokenized
#print(unique_tags) # dictionary of unique tags

from keras.preprocessing.sequence import pad_sequences as pad

# Pad sentences
def pad_sequences(train_x, train_y):
  # Pad_sequences()
  train_x = pad(train_x, padding='post', maxlen = 100)
  train_y = pad(train_y, padding='post')

  return train_x, train_y
#===============================================================================
train_x, train_y = pad_sequences(train_x, train_y)
train_x = np.asarray(train_x)
train_y = np.asarray(train_y)

#print(train_x) # train is now padded
#print(train_y) # train keys now padded

# One-hot encoding
# Transform labels to one-hot encoded labels for fully connected layer output

def to_categorical(train_y, num_tags = 9):
  one_hot = []
  new_train_y = []

  for label in train_y:
    one_hot = []
    one_hot = np.zeros((num_tags), dtype=int) # [0 0 0 0 0 0 0 0 0 0]
    one_hot[label-1] = 1  # 4 -> [0 0 0 1 0 0 0 0 0 0]
    new_train_y.append(one_hot)

  return np.asarray(new_train_y)

#===============================================================================
# Call the function as to_categorical(train_y, categories = len(tag2idx))
pd.set_option('display.max_colwidth',1000)
#print(train_y)
#print(train_y[0]) ## without one-hot [4]

train_y = to_categorical(train_y, 9)

#print(train_y[0]) ## after one-hot [0 0 0 1 0 0 0 0 0 0]

# Create model - INITIAL WITH SAVING
from keras.models import Sequential
from keras.layers import InputLayer, Activation
from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding
from scikeras.wrappers import KerasClassifier, KerasRegressor
from tensorflow.keras.optimizers import Adam

# Create model
def getModel():
  model = Sequential()
  model.add(Embedding(20000, 300, input_length=100))    # 20000 is for ~19000 vocabulary size,  300 common embedding size, max length 100 for 89 token sentence max 
  model.add(Bidirectional(LSTM(128, dropout=0.7, recurrent_dropout=0.7))) 
  model.add(Dense(9, activation='softmax'))            # 9 output layers, "labels"
  model.compile(optimizer='Nadam', loss='categorical_crossentropy', metrics=['acc'])

  return model

model = getModel()

"""### COMMENT THIS BLOCK OUT FOR DEMO

# Split dataset, train
from numpy import mean
from numpy import std
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold 
from sklearn.model_selection import cross_val_score
tf.config.experimental_run_functions_eagerly(True)

# Training - 80:20 Holdout
def train(model, train_X, train_y):
    # Fit the data into the Keras model, through 40 passes (epochs) using model.fit()
    # Set the batch size to 128, no. of iterations to 40 and validation split to 0.2.
    # Split training and validation
    model.fit(train_X, train_y, batch_size = 128, epochs = 20, validation_split = 0.2)

    # Return the model.
    return model

# Show the model architecture
model.summary()

# Train the model
model = train(model, train_x, train_y)


# method 2 - k-fold cross-validation
# Prepare cross-validation procedure
#kfold = KFold(n_splits=10, shuffle=True, random_state=1)
# Create model
#kmodel = KerasClassifier(model=getModel, epochs=5, batch_size=128, verbose=0)   # 5 epochs for testing
# Train, evaluate using 10-fold cross validation
#scores = cross_val_score(kmodel, train_x, train_y, cv=kfold)

# Save entire model to HDFS file
model.save("EC_semrela_classif3.h5")
"""

#### UNCOMMENT THIS BLOCK FOR DEMO
# Load model from HDFS file
# reference: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_load.ipynb#scrollTo=RzIOVSdnMYyO

# Recreate same model (incl weights, optimizer)
model = tf.keras.models.load_model("EC_semrela_classif3.h5") 

# Show the model architecture
model.summary()

# Analyze training and validation statistics
loss, acc = model.evaluate(train_x, train_y)
print("Restored model, Accuracy: ", acc)

# Test against other data
# Preprocess test data
def load_corpus2(filename):
  pd.set_option('display.max_rows', None)
  pd.set_option('display.max_columns', None)
  pd.set_option('display.width', 2000)

  stripped_file2 = []  # List of lines from the text file. 
  data_sentences = [] # Only "original" sentences
                        # Used as preprocessed "original" data for display later if needed.
  data_list = []  # List of lists, each sentence into a list of words
  keys_list = []  # List of keys for each sentence
  relations_list = ["Cause-Effect(e1,e2)", "Cause-Effect(e2,e1)", 
                    "Component-Whole(e1,e2)", "Component-Whole(e2,e1)", 
                    "Entity-Destination(e1,e2)", "Entity-Destination(e2,e1)",
                    "Entity-Origin(e1,e2)", "Entity-Origin(e2,e1)",
                    "Other-Relation"]

  # Convert text file into list of lines
  reader = io.open(filename)
  for line in reader:
    if not line.strip():
      continue
    else: 
      stripped_file2.append(line.strip())  # Concatenate spaces

  # Preprocess lines into data list and keys lists
  stripped_iter2 = iter(stripped_file2)

  for line in stripped_iter2:
    # 1 - Line process data (sentence)
    temp = []
    temp = line.split()[1:]   # Remove numbering in original
    temp[0] = temp[0][1:]  # Remove quotation marks from beginning of first word and end of last word
    temp[len(temp)-1] = temp[len(temp)-1][:-1]
    temp = ' '.join(temp)
    data_sentences.append(temp) # Add to list of sentences (original)

    # Clean the text
    text = temp.lower()

    text = text.replace('<e1>', ' _e11_ ')
    text = text.replace('</e1>', ' _e12_ ')
    text = text.replace('<e2>', ' _e21_ ')
    text = text.replace('</e2>', ' _e22_ ')

    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"that's", "that is ", text)
    text = re.sub(r"there's", "there is ", text)
    text = re.sub(r"it's", "it is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)

    text = text.strip()
    data_list.append(text.split())  # Split the sentence by spaces into a list and add to list of sentences

    # 2 - Line process keys
    temp2 = next(stripped_iter2)
    if temp2 not in relations_list:   # Preprocess 10 relations into 5
      temp2 = "Other-Relation"
    keys_list.append(temp2)

    # 3 - Skip comment line
    next(stripped_iter2)

  # Test original data format
  #print(data_sentences[0:10])  ##
  #print(len(data_sentences))

  return(data_sentences, data_list, keys_list)

#===============================================================================
path2 = "/content/dataset/dataset/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT"
test_sentences, test_data, test_keys = load_corpus2(path2)

#print(test_sentences[0:10]) ##
#print(test_data[0:10])  ##
#print(test_keys[0:10])  ##
#===============================================================================
# Convert sentences to unique integers
def create_dataset2(sentences, keys, unique_tags):
  # Define relevant lists
  list_x, list_y = list(), list()
  temp1, temp2 = [], []

  # Tokenize list of sentences
  for sentence in sentences:
    temp1 = []
    for word in sentence:
      if word in unique_tags:
        temp1.append(unique_tags[word])
      else:
        temp1.append(unique_tags["NA"])   # Words found in test that do not exist in training will be tagged NA
    list_x.append(temp1)

  # Tokenize keys 
  for key in keys:
    temp2 = []
    temp2.append(unique_tags[key])
    list_y.append(temp2)
    
  return np.asarray(list_x), np.asarray(list_y)
#===============================================================================
test_x, test_y  = create_dataset2(test_data, test_keys, unique_tags)

#print(test_x) ##
#print(test_y) ##
#print(unique_tags)  ##
#===============================================================================
# Pad sentences
test_x, test_y = pad_sequences(test_x, test_y)
test_x = np.asarray(test_x)
test_y = np.asarray(test_y)

#print(test_x) ##
#print(test_y) ##

#===============================================================================
# One-hot encoding of labels
test_y = to_categorical(test_y, 9)

#print(test_y) ##

# Evaluate entire test data using model
loss, acc = model.evaluate(test_x, test_y, verbose=0)
print("Loss: ", loss, " Test Accuracy: ", acc)


predictionstest = model.predict(test_x)

# Convert from logits to probability, get max class
predictions_y = []
predictions_y_onehot = []

for l in predictionstest:
  predictionstemp = tf.nn.softmax(l)
  predictions_y.append(np.argmax(predictionstemp))

# Convert one-hot encoding to labels
key_list = list(unique_tags.keys())
val_list = list(unique_tags.values())

for p in predictions_y: # For predictions
  predictions_y_onehot.append(key_list[p+1]) # Get key from the index+1


# Classification Report
print(classification_report(test_keys, predictions_y_onehot))


relations_list = ["Cause-Effect(e1,e2)", "Cause-Effect(e2,e1)", 
                  "Component-Whole(e1,e2)", "Component-Whole(e2,e1)", 
                  "Entity-Destination(e1,e2)", "Entity-Destination(e2,e1)",
                  "Entity-Origin(e1,e2)", "Entity-Origin(e2,e1)",
                  "Other-Relation"]

# Test sentences confusion matrix
# Plot confusion amtrix
print("Test Confusion Matrix  \n")
cmatrix=metrics.confusion_matrix(test_keys, predictions_y_onehot, labels=relations_list)
displ = ConfusionMatrixDisplay(confusion_matrix=cmatrix)
displ.plot(cmap="OrRd")

plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(relations_list))
plt.xticks(tick_marks, relations_list, rotation=90)
plt.yticks(tick_marks, relations_list)
plt.show()

# Seed the pseudorandom number generator
# Select 50 random samples from test set
seed(1)
use_samples = [0 , 1]
samples_to_predict = []
samples_to_predict_label = []

"""
for j in range(50):
	value = random.randint(0, len(test_x-1))
	use_samples.append(value)
#print(use_samples)  ##
"""

for k in use_samples:
  sample_sentence = test_x[k] 
  samples_to_predict.append(sample_sentence)
  samples_to_predict_label.append(test_y[k])

samples_to_predict = np.asarray(samples_to_predict)
samples_to_predict_label = np.asarray(samples_to_predict_label)


# Evaluate using the 50 test samples
data_list = []
y_list = []
y_predicted_list = []
y_predicted_list1 = []


# Accuracy / evaluation over entire test set
results = model.evaluate(samples_to_predict, samples_to_predict_label, batch_size=128)
print("Loss, Test Sample Accuracy:", results)
print("\n")

predictions = model.predict(samples_to_predict)
#print(predictions)  ## 



# Convert logits into probability
for logit in predictions:  # Logit for each word 
  predictions1 = tf.nn.softmax(logit)
  #print(predictions1)
  y_predicted_list.append(np.argmax(predictions1))

#print(y_predicted_list)

# Convert encoding to labels
for p in y_predicted_list:
  y_predicted_list1.append(key_list[p+1]) # Get key from the index+1
  
#print(y_predicted_list1) # Prediction converted back to label form

# Display results
# Obtain original sentences used in sample
for n in use_samples:
  data_list.append(test_sentences[n])
#print(data_list)

print("\n")

# Obtain original labels used in sample
for n in use_samples:
  y_list.append(test_keys[n])

# Display actual results
index = 0
for example in use_samples:
  print("Original Sentence: " + data_list[index])
  print("Actual: " + y_list[index])
  print("Predicted: " + y_predicted_list1[index])
  index += 1

# Test samples evaluation analysis

# Accuracy over 50 samples
print("Test Sample Accuracy: ", accuracy_score(np.asarray(y_list), np.asarray(y_predicted_list1)))
print("\n")

# Classification report
print(classification_report(y_list, y_predicted_list1, digits=3))


# Bar chart for predicted and actual label distributions
df2 = pd.DataFrame()    # Dataframe for actual sample
df2["Sentence"] = data_list
df2["Labels"] = y_list

df3 = pd.DataFrame()    # Dataframe for predicted sample
df3["Sentence"] = data_list
df3["Labels"] = y_predicted_list1

tfreq_df = pd.DataFrame(df2["Labels"].value_counts()) # Test Actual Relation Classes Counts total
tfreq_df.columns = ["Count"]
display(tfreq_df)
print("\n")

tfreq_df1 = pd.DataFrame(df3["Labels"].value_counts()) # Test Pred Relation Classes Counts total
tfreq_df1.columns = ["Count"]
display(tfreq_df1)
print("\n")

# Plot the bar graphs
plotdata = pd.DataFrame(tfreq_df)
plotdata.plot(kind="bar")
plt.title("Test Samples Actual Label Frequency")
plt.xlabel("Relations")
plt.ylabel("Count")

plt.show()
print("\n")

plotdata = pd.DataFrame(tfreq_df1)
plotdata.plot(kind="bar")
plt.title("Test Samples Predicted Label Frequency")
plt.xlabel("Relations")
plt.ylabel("Count")

plt.show()
print("\n")


# Create confusion matrix
# Plot confusion amtrix
print("Test Sample Confusion Matrix  \n")
cm=metrics.confusion_matrix(y_list,y_predicted_list1, labels=relations_list)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="OrRd")

plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(relations_list))
plt.xticks(tick_marks, relations_list, rotation=90)
plt.yticks(tick_marks, relations_list)
plt.show()

