# -*- coding: utf-8 -*-
"""NLP Homework 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18TBRHIf98riIppxosGB8dYVWhmjz0YSq
"""

#===============================================================================
# Emily Chen (exc160630)
# CS 6320 Natural Language Processing
# Homework 1: N-Grams
# Goal: Train and test performance of bigram language model based on training 
#   corpus for no smoothing and add-one smoothing Uses previous n-1 words to 
#   predict the probability of the next one.
# How to run: Command line arguments .\program arg1 arg2
#   arg1 - path to training corpus
#   arg2 - 0: no smoothing, 1: add-one smoothing
#===============================================================================
# pip install pandas
# pip install numpy
import pandas as pd
import numpy as np
import sys

# wordTypes - dict for each word in training corpus : freq of word
# starter_words - dict for each starter word in training corpus : freq of word
# end_words - dict for each ending word in training corpus : freq of word

# bigram_count - df for training corpus bigram 
# reduced_bigram_count - df for test sentence words (reduced from bigram_count)
# reduced_bigram_prob - df for bigram test sentence word probabilities

#-------------------------------------------------------
#  FUNCTION: calc_probTables
#   Calculates probabilities of bigram table elements
#   and displays bigram tables for test sentences
#-------------------------------------------------------
def calc_probTables(wordTypes, bigram_count, arg2, sentence1, sentence2):
  # Get list of unique tokens for test sentences
  test_sentences = sentence1 + sentence2
  
  remove = [",", ":", ";", ".", "!", "?", "[", "]", "'", "-"]
  for punct in remove:
    test_sentences = test_sentences.replace(punct, "") 
  tokenized = test_sentences.split()
  testWords = []
  for token in tokenized:
    if token not in testWords:
      testWords.append(token)
    else:
      continue

  # Reduce bigram table for only test sentence words
  reduced_bigram_count = bigram_count.loc[testWords, testWords]

  # Non-smoothing
  if arg2 == 0:                 
    print("Bigram counts table")  
    print(reduced_bigram_count)         # Display bigram count table
    reduced_bigram_prob = reduced_bigram_count.astype(float)

    # Calculate non-smoothing bigram probability
    # Iterate through each cell to calculate probability of each cell
    for i in reduced_bigram_prob.index.values:  # Rows [current]
      for j in reduced_bigram_prob.columns.values:  # Columns [next]
        # [current|next] / [total times current word occured]
        reduced_bigram_prob.at[i,j] = reduced_bigram_prob.at[i, j] / float(wordTypes[i]) 
    
    print("Bigram probabilities table")
    print(reduced_bigram_prob)

  # Add one-smoothing
  elif arg2 == 1:                 
    reduced_bigram_count += 1           # Add one
    print("Bigram counts table")
    print(reduced_bigram_count)         # Display bigram count table
    reduced_bigram_prob = reduced_bigram_count.astype(float)

    # Calculate non-smoothing bigram probability
    # Iterate through each cell to calculate probability of each cell
    for i in reduced_bigram_prob.index.values:  # Rows [current]
      for j in reduced_bigram_prob.columns.values:  # Columns [next]
        # [current|next] / ([total times current word occured] + total number of unique words)
        reduced_bigram_prob.at[i,j] = reduced_bigram_prob.at[i, j] / (float(wordTypes[i]) + len(wordTypes))

    print("Bigram probabilities table")
    print(reduced_bigram_prob)

  return reduced_bigram_prob



#-------------------------------------------------------
#  FUNCTION: sentence_prob
#   # Calculates sentence probability
#-------------------------------------------------------
def sentence_prob(starter_words, end_words, reduced_bigram_prob, sentence):
  remove = [",", ":", ";", ".", "!", "?", "[", "]", "'", "-"]
  for punct in remove:
    sentence = sentence.replace(punct, "") 
  sentenceTokenized = sentence.split()
  
  current_index = 0

  for word in sentenceTokenized: 
    if current_index == 0:             # Check probability of first word 
      if word in starter_words:
        probability = 1.0/starter_words[word] # 1/freq of word being a starter
      else:                      # Prob word being starter when never been before
        probability = 0
      prev_word = word

    else:                        # Calculate probability of words following
      # Prob of [current | next] is represented in df as [current,next] or [prev,next]
      probability = probability * (reduced_bigram_prob.at[prev_word, word])

      if (current_index == len(sentence) - 1): # Check Probability of last word
        if word in end_words:
          probability = probability * 1.0/end_words[word]
        else:
          probability = 0  

      prev_word = word
    
    current_index += 1

  return probability

#-------------------------------------------------------
#  FUNCTION: token_update
#   Updates dictionary and tables for each token
#-------------------------------------------------------
def token_update(token, prev_token, wordTypes, starter_words, bigram_count, start_sentence):
  if start_sentence:                 
    if token in wordTypes:            # Increase count if in dictionary
      wordTypes[token] = wordTypes[token] + 1 
    else: 
      wordTypes[token] = 1            # Otherwise add to dictionary
      # Add new row + columns in table filled with zeros
      zeros = [0] * len(bigram_count)
      bigram_count.loc[token] = zeros # Adds new row 
      zeros = [0] * len(bigram_count)
      bigram_count[token] = zeros     # Adds new column
    
    # Update starter words freq dict
    if token in starter_words:
      starter_words[token] = starter_words[token] + 1 
    else:
      starter_words[token] = 1         
      
  else:                               
    if token in wordTypes:            
      wordTypes[token] = wordTypes[token] + 1  
    else: 
      wordTypes[token] = 1       
      # Add new row + columns in table filled with zeros
      zeros = [0] * len(bigram_count)
      bigram_count.loc[token] = zeros # Adds new row 
      zeros = [0] * len(bigram_count)
      bigram_count[token] = zeros     # Adds new column

    # Both are start_sentence = True/False identical except for this key point
    # Update previous token + current token pairing in cell
    bigram_count.at[prev_token, token] += 1 

  return False

#-------------------------------------------------------
#  FUNCTION: main
#   Train language model using training.txt corpus
#-------------------------------------------------------
def main():
  # Test sentences
  sentence1 = "mark antony , heere take you caesars body : you shall not come to them poet ."
  sentence2 = "no , sir , there are no comets seen , the heauens speede thee in thine enterprize ."

  arg1 = sys.argv[1]
  arg2 = sys.argv[2]
  corpus = open(arg1, "r")              # Open file
  
  # Edit this for the non-smoothing/smoothing option and file name if there are issues
  #arg2 = 0
  #corpus = open("train.txt", "r")

  pd.set_option('display.max_rows', None)
  pd.set_option('display.max_columns', None)
  pd.set_option('display.width', 2000)
  #------------------------------------------------------     
  # Beginning of corpus, retrieve first line only
  # Create dictionary of word types and base table
  firstline = corpus.readline().rstrip()

  # Remove punctuation and separate on whitespace
  remove = [",", ":", ";", ".", "!", "?", "[", "]", "'", "-"]
  for punct in remove:
    firstline = firstline.replace(punct, "") 
  tokenized = firstline.split() 

  # Create initial dictionary for total word frequency
  wordTypes = {
      tokenized[0]: 1
  }
  #print(wordTypes)  

  # Create dictionary for frequency of starter words
  starter_words = {
      tokenized[0]: 1
  }

  # Create initial table
  bigram_count = pd.DataFrame(0, columns= wordTypes, index= wordTypes)
  #print(bigram_count)

  start_sentence = False
  current_index = 1
  prev_token = tokenized[0]

  # Check each token after first word in the first line
  for token in tokenized[1:]:
    token_update(token, prev_token, wordTypes, starter_words, bigram_count, start_sentence)
    prev_token = token

    # Update ending words freq dict if last word
    if (current_index == len(tokenized) - 1):
      # Create empty dictionary for frequency of ending words
      end_words = {
          token: 1
      }
      current_index = 0
    else:
      end_words = {}
      current_index += 1

  #------------------------------------------------------ 
  # Complete rest of the corpus
  # Split corpus as list of lists
  corpus_stripped1 = map(lambda s: s.strip(), corpus)
  corpus_stripped2 = (list(corpus_stripped1))

  for line in corpus_stripped2[:]: 
    remove = [",", ":", ";", ".", "!", "?", "[", "]", "'", "-"]
    for punct in remove:
      line = line.replace(punct, "") 

    # Check each token after first word in the first line
    tokenized = line.split() 
    start_sentence = True

    for token in tokenized:
      start_sentence = token_update(token, prev_token, wordTypes, starter_words, bigram_count, start_sentence)
      prev_token = token

      # Update ending words freq dict if last word
      if (current_index == len(tokenized) - 1):
        if token in end_words:
          end_words[token] = end_words[token] + 1
        else:
          end_words[token] = 1
        current_index = 0
      else:
        current_index += 1

  #print(wordTypes)
  #print(starter_words)
  #print(end_words)
  #print(bigram_count)
  #------------------------------------------------------ 
  # Evaluate bigram counts for both sentences
  reduced_bigram_prob = calc_probTables(wordTypes, bigram_count, arg2, sentence1, sentence2)

  print("Probability of sentence 1:")
  print(sentence_prob(starter_words, end_words, reduced_bigram_prob, sentence1))
  print("Probability of sentence 2:")
  print(sentence_prob(starter_words, end_words, reduced_bigram_prob, sentence2))

  corpus.close()                          # Close file
  return


#-------------------------------------------------------

if __name__ == "__main__":
  main()

